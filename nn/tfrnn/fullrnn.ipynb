{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import sys, getopt\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def stdout(s):\n",
    "    sys.stdout.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrod = 400\n",
    "nlabel = 6\n",
    "batchsize = 200\n",
    "seq_len = 3\n",
    "nEpoch = 2\n",
    "eta = 1e-2\n",
    "nInput = nrod\n",
    "nHidden = 32\n",
    "nDense = 32\n",
    "subnlayer = 1\n",
    "seqnlayer = 1\n",
    "bThetas = False\n",
    "\n",
    "bSummaries = False\n",
    "trnfile = \"/scratch/walterms/mcmd/output/xmelt/processed/xmelt_0\"\n",
    "testfile = \"/scratch/walterms/mcmd/output/xmelt/processed/xmelt_test\"\n",
    "bSaveCkpt = False\n",
    "stepsize = 1\n",
    "\n",
    "summdir = \"/home/walterms/project/walterms/mcmd/nn/tfrnn/summaries/\"\n",
    "ckptdir = \"/home/walterms/project/walterms/mcmd/nn/tfrnn/ckpts/\"\n",
    "outdir = \"/scratch/walterms/mcmd/output/xmelt/\"\n",
    "\n",
    "ckptfile = ckptdir+\"default_s10_b200_ss2\"\n",
    "\n",
    "features = [\"x\",\"y\",\"th\"]\n",
    "# features = [\"x\",\"y\",\"ft1\",\"ft2\"]\n",
    "featdict = {}\n",
    "for ft in features:\n",
    "    featdict.update({ft:[]})\n",
    "\n",
    "nchannel = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq_set(f,stepsize=1,nblMax=-1):\n",
    "\n",
    "    stdout(\"Creating sequence array from \"+f)\n",
    "\n",
    "    sortIdx = np.arange(nrod,dtype=int)\n",
    "    IDs = []\n",
    "    fin = open(f, 'r')\n",
    "    nbl = 0\n",
    "    seqset = []\n",
    "\n",
    "    writestep = 1\n",
    "    for line in fin.readlines():\n",
    "        if writestep != stepsize:\n",
    "            if line == \"\\n\":\n",
    "                writestep += 1\n",
    "            continue\n",
    "        if line == \"\\n\":\n",
    "            # Done a block\n",
    "            # Sort based on rod indices\n",
    "            sortIdx = np.argsort(IDs)\n",
    "            \n",
    "            # Insert data as triplets\n",
    "            channels = []\n",
    "            for ft in features:\n",
    "                channels.append(featdict[ft])\n",
    "            prep_data = []\n",
    "            for ch in channels:\n",
    "                prep_data.append(np.asarray(ch)[sortIdx])\n",
    "            formatted_data = np.stack(prep_data)\n",
    "            seqset.append(formatted_data)\n",
    "                \n",
    "            for ft in features:\n",
    "                featdict[ft] = []\n",
    "            IDs = []\n",
    "            nbl+=1\n",
    "            writestep = 1\n",
    "            if nbl == nblMax:\n",
    "                break\n",
    "            continue\n",
    "        spt = [float(x) for x in line.split()]\n",
    "        featdict[\"x\"].append(spt[0]-0.5)\n",
    "        featdict[\"y\"].append(spt[1]-0.5)\n",
    "        th = spt[2]-0.5\n",
    "        featdict[\"th\"].append(th)\n",
    "        \n",
    "        IDs.append(int(spt[3]))\n",
    "\n",
    "    fin.close()\n",
    "\n",
    "    return np.asarray(seqset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    #A ttach a lot of summaries to a Tensor (for TensorBoard visualization)\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#   STATE  RNN    # \n",
    "###################\n",
    "\n",
    "stdout(\"Creating State RNN graph\")\n",
    "\n",
    "X_st = tf.placeholder(\"float\", [None, nchannel, nInput])\n",
    "Y_st = tf.placeholder(\"float\", [None, nlabel])\n",
    "\n",
    "with tf.name_scope('dense_st'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#    SEQ  RNN     # \n",
    "###################\n",
    "\n",
    "stdout(\"Creating Seq RNN graph\")\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, seq_len, nchannel, nInput],name=\"X\")\n",
    "Y = tf.placeholder(\"float\", [None, nchannel, nInput],name=\"Y\")\n",
    "\n",
    "with tf.name_scope('dense'):\n",
    "    dense_weights = {\"pre\":tf.Variable(tf.random_normal([nHidden,nDense],\n",
    "                stddev=0.1,dtype=tf.float32),name=\"pre_w\")}\n",
    "    for f in features:\n",
    "        dense_weights.update({f:tf.Variable(tf.random_normal([nDense,nrod],\n",
    "                stddev=0.1,dtype=tf.float32),name=f+\"_w\")})\n",
    "\n",
    "    dense_biases = {\"pre\":tf.Variable(tf.random_normal([nDense],\n",
    "                stddev=0.1,dtype=tf.float32),name=\"pre_b\")}\n",
    "    for f in features:\n",
    "        dense_biases.update({f:tf.Variable(tf.random_normal([nrod],\n",
    "                stddev=0.1,dtype=tf.float32),name=f+\"_b\")})\n",
    "        \n",
    "    for w in dense_weights:\n",
    "        tf.summary.histogram(w+\"_ws\",dense_weights[w])\n",
    "    for b in dense_biases:\n",
    "        tf.summary.histogram(b+\"_bs\",dense_biases[b])\n",
    "\n",
    "\n",
    "# Define an lstm cell with tensorflow\n",
    "def lstm_cell(nUnits):\n",
    "    return rnn.BasicLSTMCell(nUnits)\n",
    "\n",
    "def seqRNN(x):\n",
    "\n",
    "    x = tf.unstack(x,seq_len,1) # unstack along time dimension\n",
    "    \n",
    "    with tf.name_scope('subrnn'):\n",
    "        with tf.variable_scope('subrnn'):\n",
    "            # Subcell    \n",
    "#             subcell = lstm_cell(nHidden)\n",
    "            subcell = rnn.MultiRNNCell([lstm_cell(nHidden) for _ in range(subnlayer)])\n",
    "\n",
    "            suboutputs = []\n",
    "            substate = subcell.zero_state(batchsize,tf.float32)\n",
    "\n",
    "            # Loop over the images in a sequence\n",
    "            for x_img in x:\n",
    "                x_ = tf.unstack(x_img,nchannel,1)\n",
    "                # Returns multiple outputs I think of size [batchsize,nchannel,subcell.output_size]\n",
    "                suboutput_img, substate = tf.nn.static_rnn(subcell,x_,dtype=tf.float32,initial_state=substate)\n",
    "                # suboutput_img is a list of 3 outputs from each iteration on the img\n",
    "                # suboutput_img[-1] is the last output, let's use that as input to the seqrnn\n",
    "                suboutputs.append(suboutput_img[-1])\n",
    "\n",
    "            tf.summary.histogram('substate',substate)\n",
    "\n",
    "    with tf.name_scope('seqrnn'):\n",
    "        with tf.variable_scope('seqrnn'):\n",
    "            # Main cell\n",
    "#             cell = lstm_cell(nHidden)\n",
    "            cell = rnn.MultiRNNCell([lstm_cell(nHidden) for _ in range(seqnlayer)])\n",
    "\n",
    "            outputs,state = tf.nn.static_rnn(cell,suboutputs,dtype=tf.float32)\n",
    "            tf.summary.histogram('cellstate',state)\n",
    "\n",
    "\n",
    "    # Dense output from seqrnn\n",
    "    with tf.name_scope('dense'):\n",
    "        dense_pre = tf.nn.elu(tf.add(tf.matmul(outputs[-1],dense_weights[\"pre\"]),\n",
    "                        dense_biases[\"pre\"]),name=\"pre_out_activ\")\n",
    "\n",
    "        # Tensors for transforming output of main RNN unit into an img\n",
    "        out_img_channels = []\n",
    "        i = 0\n",
    "        for ft in features:\n",
    "            out_img_channels.append(tf.nn.tanh(tf.add(tf.matmul(\n",
    "                dense_pre,dense_weights[ft]),dense_biases[ft]),name=str(ft)+\"_out_activ\"))\n",
    "\n",
    "            tf.summary.histogram(str(ft)+\"_out\",out_img_channels[-1])\n",
    "            i+=1\n",
    "    \n",
    "    return tf.stack(out_img_channels,axis=1)\n",
    "\n",
    "\n",
    "# Outputs a list of tensors of size nrod representing the img\n",
    "seq_img = seqRNN(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=seq_img))\n",
    "tf.summary.scalar('loss',loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=eta).minimize(loss)\n",
    "\n",
    "stdout(\"Finished Graph\")\n",
    "\n",
    "\n",
    "###################\n",
    "#    TRAINING     # \n",
    "###################\n",
    "\n",
    "# Saver for checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochEval = int(10**(np.log10(nEpoch)//1 - 1))\n",
    "if epochEval<1: epochEval=1\n",
    "\n",
    "outlosses = []\n",
    "floss = open(lossname, 'w')\n",
    "\n",
    "# Generate training list\n",
    "trnlist = [trnfile]\n",
    "if trnrange:\n",
    "    trnlist = []\n",
    "    first, last = int(trnrange[0]),int(trnrange[1])\n",
    "    for i in range(last-first+1):\n",
    "        trnlist.append(trnfile+\"_\"+str(i))\n",
    "\n",
    "stdout(\"Generating testing data...\")\n",
    "test_seq = gen_seq_set(testfile,stepsize=stepsize)\n",
    "stdout(\"Done\")\n",
    "nTestSeq = len(test_seq)-seq_len\n",
    "nTestSample = (600//batchsize + 1)*batchsize\n",
    "ntestbatches = nTestSample//batchsize\n",
    "imgIdx_test = [i for i in range(nTestSeq)]\n",
    "nextra_test = ntestbatches*batchsize - nTestSeq\n",
    "stdout(str(len(test_seq))+\" images in test set\")\n",
    "stdout(str(batchsize*ntestbatches)+\" sequences per epoch\")\n",
    "\n",
    "stdout(\"Beginning Session\")\n",
    "with tf.Session() as sess:\n",
    "    if bSummaries:\n",
    "        summaries = tf.summary.merge_all()\n",
    "        now = time.localtime()\n",
    "        writeto = summdir+time.strftime(\"%Y%m%d-%H%M%S\",now) + \"/\"\n",
    "        train_writer = tf.summary.FileWriter(writeto+\"train\", sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(writeto+\"test\")\n",
    "\n",
    "    # Checkpoint file\n",
    "    if ckptfile:\n",
    "        ckptfile = ckptdir+ckptfile\n",
    "        stdout(\"Restoring from \"+ckptfile)\n",
    "        saver.restore(sess, ckptfile)\n",
    "        stdout(\"Model restored\")\n",
    "    else:\n",
    "        stdout(\"No checkpoint file given for model restore\")\n",
    "        ckptfile = ckptdir+\"default_s\"+str(seq_len)+\"_b\"+str(batchsize)+\"_ss\"+str(stepsize)+\".ckpt\"\n",
    "        stdout(\"Initializing variables\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tstart = time.time()\n",
    "    trnstep = 0\n",
    "\n",
    "    # Train over the training list\n",
    "    for trnf in trnlist:\n",
    "        # Generate seq sets\n",
    "        stdout(\"Generating seq from \"+trnf+\"...\")\n",
    "        try:\n",
    "            trn_seq = gen_seq_set(trnf,stepsize=stepsize)\n",
    "        except:\n",
    "            stdout(\"Failed to generate sequence, trying next file\")\n",
    "            continue\n",
    "        stdout(\"Done\")\n",
    "\n",
    "        nTrnSeq = len(trn_seq)-seq_len\n",
    "\n",
    "        # Add +1 to batches per\n",
    "        batchesPerEpoch = nTrnSeq//batchsize\n",
    "\n",
    "        imgIdx_trn = [i for i in range(nTrnSeq)]\n",
    "\n",
    "        stdout(str(len(trn_seq))+\" images in train set\")\n",
    "        stdout(str(batchesPerEpoch*batchsize)+\" sequences per epoch\")\n",
    "\n",
    "        metarecord_ib = int(10**(np.log10(batchesPerEpoch)//1 - 1))\n",
    "        if metarecord_ib<1: metarecord_ib=1\n",
    "    \n",
    "        for e in range(nEpoch):\n",
    "            trn_loss = 0.\n",
    "            random.shuffle(imgIdx_test)\n",
    "            random.shuffle(imgIdx_trn)\n",
    "            start = 0\n",
    "            for ib in range(batchesPerEpoch):\n",
    "                trnstep += 1\n",
    "                # Pepare a batch\n",
    "                end = start+batchsize\n",
    "                yin = np.asarray([trn_seq[i_img+seq_len] \\\n",
    "                        for i_img in imgIdx_trn[start:end]])\n",
    "                xin = np.asarray([[trn_seq[i_img+s] for s in range(seq_len)] \\\n",
    "                        for i_img in imgIdx_trn[start:end]])\n",
    "                \n",
    "                start = end\n",
    "                \n",
    "                if ib%metarecord_ib==0 and bSummaries:\n",
    "                    # Record run metadata\n",
    "                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "\n",
    "                    _,l,S = sess.run([optimizer, loss, summaries], feed_dict={X: xin, Y: yin},\n",
    "                                    options=run_options, run_metadata=run_metadata)\n",
    "\n",
    "                    train_writer.add_run_metadata(run_metadata, \"step %d\"%(trnstep))\n",
    "                    train_writer.add_summary(S, trnstep)\n",
    "                    \n",
    "                else:\n",
    "                    _,l = sess.run([optimizer,loss], feed_dict={X:xin,Y:yin})\n",
    "\n",
    "                trn_loss += l / batchesPerEpoch\n",
    "                \n",
    "            if e % epochEval == 0:\n",
    "                # Eval on test set\n",
    "                test_loss = 0.\n",
    "                start = 0\n",
    "                for tb in range(ntestbatches):\n",
    "                    end = start+batchsize\n",
    "                    yin = np.asarray([test_seq[i_img+seq_len] \\\n",
    "                            for i_img in imgIdx_test[start:end]])\n",
    "                    xin = np.asarray([[test_seq[i_img+s] for s in range(seq_len)] \\\n",
    "                            for i_img in imgIdx_test[start:end]])\n",
    "                    l, = sess.run([loss],feed_dict={X:xin,Y:yin})\n",
    "                    test_loss += l/ntestbatches\n",
    "\n",
    "                    start = end\n",
    "\n",
    "                tend = time.time()\n",
    "                stdout(\"(t\"+str(trnstep)+\") epoch \"+str(e)+\"  trn_loss \"+'%.6f'%(trn_loss)+\\\n",
    "                    \"  test_loss \"+'%.6f'%(test_loss)+\"  elapsed time(s) \"+str((tend-tstart)))\n",
    "                outlosses.append((trnstep,trn_loss))\n",
    "                tstart = time.time()\n",
    "        \n",
    "    stdout(\"Done Training\")\n",
    "\n",
    "    \n",
    "    if bSaveCkpt:\n",
    "        # Saving checkpoint\n",
    "        stdout(\"Saving checkpoint to \"+ckptfile)\n",
    "        save_path = saver.save(sess, ckptfile)\n",
    "        stdout(\"Saved checkpoint\")\n",
    "\n",
    "    if bSummaries:\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "\n",
    "    \n",
    "floss.write(\"nEpoch %d | batchsize %d | nTrn %d | nTest %d | SeqLen %d | eta %.5f | nHidden %d | nDense %d | subnlayer %d | seqnlayer %d\\n\"%(nEpoch,batchsize,len(trn_seq),len(test_seq),seq_len,eta,nHidden,nDense,subnlayer,seqnlayer))\n",
    "\n",
    "floss.write(\"data header: trnstep trn_loss\")\n",
    "for t,l in outlosses:\n",
    "    floss.write(\"%d %f\\n\"%(int(t),l))\n",
    "sess.close()\n",
    "\n",
    "global_tend = time.time()\n",
    "stdout(\"Total time: \"+str(global_tend-global_tstart))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
