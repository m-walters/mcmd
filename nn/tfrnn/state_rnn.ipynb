{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePerType = nblPerTrnFile + nblPerTestFile\n",
    "data0 = 0.8*np.random.uniform(size=(samplePerType,nchannel,nrod))\n",
    "# data0 = 1.0*np.random.triangular(0.,0.5,1.0,size=(samplePerType,nrod\n",
    "data1 = 1.0*np.random.triangular(0.,0.5,1.0,size=(samplePerType,nchannel,nrod))\n",
    "\n",
    "set0 = np.asarray([[data0[i],np.asarray([1.,0.,0.,0.,0.,0.])] for i in range(samplePerType)])\n",
    "set1 = np.asarray([[data1[i],np.asarray([0.,1.,0.,0.,0.,0.])] for i in range(samplePerType)])\n",
    "trn_data = np.concatenate((set0[:nblPerTrnFile],set1[:nblPerTrnFile]))\n",
    "test_data = np.concatenate((set0[nblPerTrnFile:],set1[nblPerTrnFile:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%source bin/activate\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rundir = \"/home/walterms/project/walterms/mcmd/nn/data/\"\n",
    "trndir = rundir+\"train/\"\n",
    "testdir = rundir+\"test/\"\n",
    "unlbldir = rundir+\"unlbl/\"\n",
    "bThetas = False\n",
    "nrod = 400\n",
    "nlabel = 6\n",
    "\n",
    "nchannel = 3\n",
    "if bThetas: nchannel = 1\n",
    "\n",
    "nblPerTrnFile = 5000\n",
    "nblPerTestFile = 300\n",
    "\n",
    "# trnlist = [\"tjam\", \"xjam\", \"ujam\", \"ljam\", \"edge15.00\", \"edge30.00\"]\n",
    "trnlist = [\"edge15.00\", \"edge30.00\"]\n",
    "# testlist = [\"tjam\", \"xjam\", \"ujam\", \"ljam\", \"edge15.00\", \"edge30.00\"]\n",
    "testlist = [\"edge15.00\", \"edge30.00\"]\n",
    "unlbllist = [\"xmelt\"]\n",
    "\n",
    "trnlist = [trndir+x for x in trnlist]\n",
    "testlist = [testdir+x for x in testlist]\n",
    "unlbllist = [unlbldir+x for x in unlbllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/walterms/project/walterms/mcmd/nn/data/train/edge15.00 as training data\n",
      "Processing /home/walterms/project/walterms/mcmd/nn/data/train/edge30.00 as training data\n",
      "Done compiling training set, 10000 images\n"
     ]
    }
   ],
   "source": [
    "# Compile training set\n",
    "trn_data = []\n",
    "\n",
    "shufIdx = [p for p in range(nrod)]\n",
    "random.shuffle(shufIdx)\n",
    "            \n",
    "for f in trnlist:\n",
    "    print \"Processing \" + f + \" as training data\"\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    fin = open(f, 'r')\n",
    "    nTrn = 0\n",
    "    for line in fin.readlines():\n",
    "        if line == \"\\n\": continue\n",
    "        if line.startswith(\"label\"):\n",
    "            # Done a block\n",
    "            lbls = np.zeros((nlabel))\n",
    "            lbl = int(float(line.split()[1]))\n",
    "            lbls[lbl] = 1.\n",
    "            \n",
    "            if bThetas:\n",
    "                thdata = None\n",
    "                random.shuffle(thetas)\n",
    "                thdata = (-0.5) + np.asarray(thetas)\n",
    "                formatted_data = np.stack([thdata,thdata,thdata])\n",
    "#                 thdata = thdata.reshape(1,nrod)\n",
    "                trn_data.append([formatted_data, lbls])\n",
    "\n",
    "                \n",
    "            else:\n",
    "                # Insert data as triplets\n",
    "                channels = [xs,ys,thetas]\n",
    "                prep_data = []\n",
    "                for ch in channels:\n",
    "                    prep_data.append((-0.5)+np.asarray(ch))\n",
    "                formatted_data = np.stack(prep_data)\n",
    "#                 formatted_data = np.stack([np.asarray(xs),np.asarray(ys),np.asarray(thetas)])\n",
    "                np.random.shuffle(formatted_data)\n",
    "                trn_data.append([formatted_data, lbls])\n",
    "                \n",
    "            thetas = []\n",
    "            xs = []\n",
    "            ys = []\n",
    "            nTrn+=1\n",
    "            if nTrn == nblPerTrnFile: \n",
    "                break\n",
    "            continue\n",
    "        spt = [float(x) for x in line.split()]\n",
    "        xs.append(spt[0])\n",
    "        ys.append(spt[1])\n",
    "        thetas.append(spt[2])\n",
    "\n",
    "    fin.close()\n",
    "    \n",
    "trn_data = np.asarray(trn_data)\n",
    "    \n",
    "print \"Done compiling training set,\", len(trn_data), \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /home/walterms/project/walterms/mcmd/nn/data/test/edge15.00 to test set\n",
      "Adding /home/walterms/project/walterms/mcmd/nn/data/test/edge30.00 to test set\n",
      "Done compiling test set, 600 images\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "\n",
    "for f in testlist:\n",
    "    print \"Adding \" + f + \" to test set\"\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    fin = open(f, 'r')\n",
    "    nbl = 0\n",
    "    for line in fin.readlines():\n",
    "        if line == \"\\n\": continue\n",
    "        if line.startswith(\"label\"):\n",
    "            # Done a block\n",
    "            lbls = np.zeros((nlabel))\n",
    "            lbl = int(float(line.split()[1]))\n",
    "            lbls[lbl] = 1.\n",
    "            if bThetas:\n",
    "                random.shuffle(thetas)\n",
    "                thdata = (-0.5) + np.asarray(thetas)\n",
    "                formatted_data = np.stack([thdata,thdata,thdata])\n",
    "#                 thdata = thdata.reshape(1,nrod)\n",
    "                test_data.append([formatted_data, lbls])\n",
    "\n",
    "            else:\n",
    "                # Insert data as triplets\n",
    "                channels = [xs,ys,thetas]\n",
    "                prep_data = []\n",
    "                for ch in channels:\n",
    "                    prep_data.append((-0.5)+np.asarray(ch))\n",
    "                formatted_data = np.stack(prep_data)\n",
    "#                 formatted_data = np.stack([np.asarray(xs),np.asarray(ys),np.asarray(thetas)])\n",
    "                np.random.shuffle(formatted_data)\n",
    "                test_data.append([formatted_data, lbls])\n",
    "            thetas = []\n",
    "            xs = []\n",
    "            ys = []\n",
    "            nbl+=1\n",
    "            if nbl == nblPerTestFile: \n",
    "                break\n",
    "            continue\n",
    "        spt = [float(x) for x in line.split()]\n",
    "        xs.append(spt[0])\n",
    "        ys.append(spt[1])\n",
    "        thetas.append(spt[2])\n",
    "\n",
    "    fin.close()\n",
    "\n",
    "test_data = np.asarray(test_data)\n",
    "\n",
    "print \"Done compiling test set,\", len(test_data), \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 600\n"
     ]
    }
   ],
   "source": [
    "print len(trn_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([trn_data[0][0][0],trn_data[-1][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#       RNN       # \n",
    "###################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "eta = 1e-4\n",
    "batchsize = 500\n",
    "nInput = nrod\n",
    "nHidden = 256\n",
    "nLayer = 1\n",
    "\n",
    "sizedict = {\"nchannel\": nchannel,\n",
    "            \"batchsize\": batchsize,\n",
    "            \"nHidden\": nHidden,\n",
    "            \"nLayer\": nLayer,\n",
    "            \"nlabel\": nlabel}\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, nchannel, nInput])\n",
    "Y = tf.placeholder(\"float\", [None, nlabel])\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([nHidden, nHidden],stddev=0.1, dtype=tf.float32), name='W1'),\n",
    "    'out': tf.Variable(tf.random_normal([nHidden, nlabel],stddev=0.1, dtype=tf.float32), name='W'),\n",
    "    \n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([nHidden],stddev=0.1, dtype=tf.float32), name='b1'),\n",
    "    'out': tf.Variable(tf.random_normal([nlabel],stddev=0.1, dtype=tf.float32), name='bias')\n",
    "}\n",
    "\n",
    "\n",
    "# Define an lstm cell with tensorflow\n",
    "def lstm_cell(nUnits):\n",
    "    return rnn.BasicLSTMCell(nUnits)\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    # Build high lvl rnn for time series\n",
    "    x = tf.unstack(x, nchannel, 1)\n",
    "    cell = lstm_cell(nHidden)\n",
    "    stack = rnn.MultiRNNCell([lstm_cell(nHidden) for _ in range(nLayer)])\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(stack, x, dtype=tf.float32)\n",
    "    layer_trans = tf.add(tf.matmul(outputs[-1], weights['h1']), biases['b1'])\n",
    "    layer_trans = tf.nn.sigmoid(layer_trans)\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(layer_trans, weights['out']) + biases['out']\n",
    "\n",
    "###############################\n",
    "# Save structure that works\n",
    "# def RNN(x, weights, biases):\n",
    "#     # Lower level RNN structure to learn classifications\n",
    "#     x = tf.unstack(x, nchannel, 1)\n",
    "#     cell = lstm_cell(nHidden)\n",
    "#     stack = rnn.MultiRNNCell([lstm_cell(nHidden) for _ in range(nLayer)])\n",
    "\n",
    "#     # Get lstm cell output\n",
    "#     outputs, states = rnn.static_rnn(stack, x, dtype=tf.float32)\n",
    "#     layer_trans = tf.add(tf.matmul(outputs[-1], weights['h1']), biases['b1'])\n",
    "#     layer_trans = tf.nn.sigmoid(layer_trans)\n",
    "#     # Linear activation, using rnn inner loop last output\n",
    "#     return tf.matmul(layer_trans, weights['out']) + biases['out']\n",
    "###############################\n",
    "\n",
    "\n",
    "logits = RNN(X,weights,biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=eta).minimize(loss_op)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch = 200\n",
    "epochEval = 20\n",
    "batchesPerEpoch = int(len(trn_data)/batchsize)\n",
    "ntestbatches = int(len(test_data)/batchsize)\n",
    "\n",
    "testdata_tmp = [t for t in test_data]\n",
    "random.shuffle(testdata_tmp)\n",
    "xtest = np.asarray([t[0] for t in testdata_tmp])\n",
    "ytest = np.asarray([t[1] for t in testdata_tmp])\n",
    "y_out_0 = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    testy = sess.run([teststate],feed_dict={X: xtest[0:batchsize], Y: ytest[0:batchsize]})\n",
    "    \n",
    "    print testy\n",
    "    print np.asarray(testy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runrecord:\n",
    "    def __init__(self,\n",
    "                trnlist,\n",
    "                testlist,\n",
    "                bThetas,\n",
    "                sizedict,\n",
    "                batchsize,\n",
    "                eta,\n",
    "                nEpoch):\n",
    "        self.params = {\n",
    "            \"trnlist\":trnlist,\n",
    "            \"testlist\":testlist,\n",
    "            \"bThetas\":bThetas,\n",
    "            \"batchsize\":batchsize,\n",
    "            \"eta\":eta,\n",
    "            \"nEpoch\":nEpoch\n",
    "        }\n",
    "        \n",
    "        self.sizedict=sizedict\n",
    "        self.testacc = np.zeros(shape=(nEpoch,))        \n",
    "        self.trnacc = np.zeros(shape=(nEpoch,))        \n",
    "        self.testlosses = np.zeros(shape=(nEpoch,))        \n",
    "        self.trnlosses = np.zeros(shape=(nEpoch,))\n",
    "        \n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove most recent run from records\n",
    "tmp = list(records)\n",
    "records = []\n",
    "for r in tmp[0:-1]:\n",
    "    records.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, test accuracy 0.442, testloss 1.60948, avg_loss 1.76226\n",
      "epoch 5, test accuracy 0.664, testloss 0.746973, avg_loss 0.786754\n",
      "epoch 10, test accuracy 0.782, testloss 0.628801, avg_loss 0.641451\n",
      "epoch 15, test accuracy 0.85, testloss 0.484271, avg_loss 0.512528\n",
      "epoch 20, test accuracy 0.896, testloss 0.350794, avg_loss 0.37645\n",
      "epoch 25, test accuracy 0.928, testloss 0.256616, avg_loss 0.269372\n",
      "epoch 30, test accuracy 0.948, testloss 0.18865, avg_loss 0.20087\n",
      "epoch 35, test accuracy 0.972, testloss 0.135453, avg_loss 0.145896\n",
      "epoch 40, test accuracy 0.984, testloss 0.101041, avg_loss 0.107767\n",
      "epoch 45, test accuracy 0.992, testloss 0.0753365, avg_loss 0.0800358\n",
      "epoch 50, test accuracy 0.996, testloss 0.0588638, avg_loss 0.0601635\n",
      "epoch 55, test accuracy 0.996, testloss 0.0475139, avg_loss 0.046728\n",
      "epoch 60, test accuracy 0.996, testloss 0.0386507, avg_loss 0.0373812\n",
      "epoch 65, test accuracy 0.998, testloss 0.0334067, avg_loss 0.031622\n",
      "epoch 70, test accuracy 0.996, testloss 0.0292173, avg_loss 0.0252575\n",
      "epoch 75, test accuracy 0.998, testloss 0.0252046, avg_loss 0.02124\n",
      "epoch 80, test accuracy 0.998, testloss 0.0224416, avg_loss 0.0178998\n",
      "epoch 85, test accuracy 0.998, testloss 0.0199895, avg_loss 0.0151094\n",
      "epoch 90, test accuracy 0.998, testloss 0.0188027, avg_loss 0.013867\n",
      "epoch 95, test accuracy 0.998, testloss 0.0170308, avg_loss 0.011135\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "nEpoch = 100\n",
    "epochEval = 5\n",
    "batchesPerEpoch = int(len(trn_data)/batchsize)\n",
    "ntestbatches = int(len(test_data)/batchsize)\n",
    "\n",
    "testdata_tmp = [t for t in test_data]\n",
    "random.shuffle(testdata_tmp)\n",
    "xtest = np.asarray([t[0] for t in testdata_tmp])\n",
    "ytest = np.asarray([t[1] for t in testdata_tmp])\n",
    "y_out_0 = []\n",
    "\n",
    "thisrecord = runrecord(trnlist,testlist,bThetas,sizedict,batchsize,eta,nEpoch)\n",
    "records.append(thisrecord)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "with sess.as_default():\n",
    "    assert tf.get_default_session() is sess\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    for e in range(nEpoch):\n",
    "        avg_loss = 0.\n",
    "        batchIdx = [bb for bb in range(batchesPerEpoch)]\n",
    "        random.shuffle(batchIdx)\n",
    "        for b in batchIdx:\n",
    "            ib = b*batchsize\n",
    "            # Pepare a batch\n",
    "            yin = np.asarray([trn_data[ib+iib][1] for iib in xrange(batchsize)])\n",
    "            xin = np.asarray([trn_data[ib+iib][0] for iib in xrange(batchsize)])\n",
    "\n",
    "            _,l = sess.run([optimizer, loss_op], feed_dict={X: xin, Y: yin})\n",
    "            # Run optimization op (backprop)\n",
    "            avg_loss += l / batchesPerEpoch\n",
    "            \n",
    "        if e % epochEval == 0:\n",
    "            # Eval on test set\n",
    "            avg_testloss = 0.\n",
    "            testacc = 0.\n",
    "            for tb in range(ntestbatches):\n",
    "                ib = tb*batchsize\n",
    "                xin = xtest[ib:ib+batchsize]\n",
    "                yin = ytest[ib:ib+batchsize]\n",
    "                acc, youts, testloss = sess.run([accuracy,prediction,loss_op],feed_dict={\n",
    "                    X: xin, Y: yin})\n",
    "                testacc += acc/float(ntestbatches)\n",
    "                avg_testloss += testloss/float(ntestbatches)\n",
    "            print('epoch %d, test accuracy %g, testloss %g, avg_loss %g'\n",
    "                 % (e,testacc,avg_testloss,avg_loss))\n",
    "            y_out_0.append(youts[0])\n",
    "\n",
    "#             thisrecord.testlosses[e] = testcost\n",
    "#             thisrecord.trnlosses[e] = trncost\n",
    "#             thisrecord.testacc[e] = test_accuracy\n",
    "#             thisrecord.trnacc[e] = trnacc\n",
    "            \n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/walterms/project/walterms/mcmd/nn/data/unlbl/xmelt as training data\n",
      "Done compiling unlabeled set, 10001 images\n"
     ]
    }
   ],
   "source": [
    "# Compile unlabeled data\n",
    "unlbl_data = []\n",
    "maxUnlbl = -1\n",
    "            \n",
    "for f in unlbllist:\n",
    "    print \"Processing \" + f + \" as training data\"\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    fin = open(f, 'r')\n",
    "    nbl = 0\n",
    "    for line in fin.readlines():\n",
    "        if line == \"\\n\":\n",
    "            # Done a block\n",
    "            if bThetas:\n",
    "                thdata = None\n",
    "                random.shuffle(thetas)\n",
    "                thdata = (-0.5) + np.asarray(thetas)\n",
    "                formatted_data = np.stack([thdata,thdata,thdata])\n",
    "#                 thdata = thdata.reshape(1,nrod)\n",
    "                trn_data.append([formatted_data, lbls])\n",
    "\n",
    "            else:\n",
    "                # Insert data as triplets\n",
    "                channels = [xs,ys,thetas]\n",
    "                prep_data = []\n",
    "                for ch in channels:\n",
    "                    prep_data.append((-0.5)+np.asarray(ch))\n",
    "                formatted_data = np.stack(prep_data)\n",
    "#                 formatted_data = np.stack([np.asarray(xs),np.asarray(ys),np.asarray(thetas)])\n",
    "                np.random.shuffle(formatted_data)\n",
    "                unlbl_data.append([formatted_data, lbls])\n",
    "                \n",
    "            thetas = []\n",
    "            xs = []\n",
    "            ys = []\n",
    "            nbl+=1\n",
    "            if nbl == maxUnlbl: \n",
    "                break\n",
    "            continue\n",
    "        spt = [float(x) for x in line.split()]\n",
    "        xs.append(spt[0])\n",
    "        ys.append(spt[1])\n",
    "        thetas.append(spt[2])\n",
    "\n",
    "    fin.close()\n",
    "    \n",
    "unlbl_data = np.asarray(unlbl_data)\n",
    "    \n",
    "print \"Done compiling unlabeled set,\", len(unlbl_data), \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with sess.as_default():\n",
    "    assert tf.get_default_session() is sess\n",
    "    # Pepare a batch\n",
    "    xin = np.asarray([trn_data[ib+iib][0] for iib in xrange(batchsize)])\n",
    "\n",
    "    _,l = sess.run([optimizer, loss_op], feed_dict={X: xin, Y: yin})\n",
    "    # Run optimization op (backprop)\n",
    "    avg_loss += l / batchesPerEpoch\n",
    "\n",
    "    if e % epochEval == 0:\n",
    "        # Eval on test set\n",
    "        avg_testloss = 0.\n",
    "        testacc = 0.\n",
    "        for tb in range(ntestbatches):\n",
    "            ib = tb*batchsize\n",
    "            xin = xtest[ib:ib+batchsize]\n",
    "            yin = ytest[ib:ib+batchsize]\n",
    "            acc, youts, testloss = sess.run([accuracy,prediction,loss_op],feed_dict={\n",
    "                X: xin, Y: yin})\n",
    "            testacc += acc/float(ntestbatches)\n",
    "            avg_testloss += testloss/float(ntestbatches)\n",
    "        print('epoch %d, test accuracy %g, testloss %g, avg_loss %g'\n",
    "             % (e,testacc,avg_testloss,avg_loss))\n",
    "        y_out_0.append(youts[0])\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "# Mask on zeros\n",
    "f, ax = plt.subplots(1,2)\n",
    "for r in records[-1:]:\n",
    "    x = np.argwhere(r.testlosses>0)\n",
    "    ax0, = ax[0].plot(x,r.trnlosses[x],'b-',label='train loss')\n",
    "    axtwin = ax[0].twinx()\n",
    "    ax0t, = axtwin.plot(x,r.testlosses[x],'g:',label='test loss')\n",
    "    l1 = plt.legend([ax0,ax0t],[\"train loss\", \"test loss\"])\n",
    "    ax[1].plot(x,r.trnacc[x],'b-',label=\"train\")\n",
    "    ax[1].plot(x,r.testacc[x],'g:',label=\"test\")\n",
    "    ax[1].legend()\n",
    "    ax[0].set_yscale(\"log\", nonposy='mask')\n",
    "    axtwin.set_yscale(\"log\", nonposy='mask')\n",
    "    \n",
    "plt.gcf().set_size_inches(14,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
